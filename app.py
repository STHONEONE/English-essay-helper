import nltk

import streamlit as st
import re
from textblob import TextBlob
from collections import Counter
import nltk
from nltk.corpus import wordnet


# --- 0. ç¯å¢ƒé…ç½®ä¸æ•°æ®ä¸‹è½½ ---
@st.cache_resource
def download_nltk_data():
    resources = ['wordnet', 'omw-1.4', 'punkt', 'averaged_perceptron_tagger']
    for resource in resources:
        try:
            nltk.data.find(f'corpora/{resource}')
        except LookupError:
            nltk.download(resource, quiet=True)


download_nltk_data()

# --- 1. é¡µé¢é…ç½® ---
st.set_page_config(page_title="Essay Optimizer AI Final", layout="wide", page_icon="ğŸ“")

st.title("ğŸ“ AI è‹±è¯­ä½œæ–‡æ·±åº¦ä¼˜åŒ–åŠ©æ‰‹")
st.markdown("æ ¸å¿ƒåŠŸèƒ½ï¼š**æ™ºèƒ½è¯æ±‡åˆ—è¡¨**  + **é•¿éš¾å¥è‡ªåŠ¨æ‹†åˆ†**")

# --- åœç”¨è¯ ---
STOP_WORDS = {
    'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with',
    'by', 'from', 'up', 'about', 'into', 'over', 'after', 'is', 'are', 'was', 'were',
    'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'it', 'this',
    'that', 'these', 'those', 'i', 'you', 'he', 'she', 'we', 'they', 'my', 'your',
    'his', 'her', 'our', 'their', 'me', 'him', 'us', 'them', 'so', 'very', 'really'
}


# --- æ ¸å¿ƒå‡½æ•° ---

def get_synonyms(word):
    """è·å–åŒä¹‰è¯"""
    synonyms = set()
    for syn in wordnet.synsets(word):
        for l in syn.lemmas():
            cleaned_syn = l.name().replace('_', ' ')
            if cleaned_syn.lower() != word.lower():
                synonyms.add(cleaned_syn)
    return list(synonyms)[:5]


def smart_split_sentence(sentence):
    """
    é•¿éš¾å¥æ‹†åˆ†ç®—æ³• (ä¿ç•™ v3.1 çš„é€»è¾‘)
    """
    split_pattern = r'(,\s*(?:but|and|so|because|although|since|while))'
    parts = re.split(split_pattern, sentence, flags=re.IGNORECASE)

    if len(parts) == 1: return None

    refined_sentences = []
    current_sent = parts[0]

    for i in range(1, len(parts), 2):
        separator = parts[i]
        next_part = parts[i + 1]
        conjunction = re.sub(r'[,\s]', '', separator)
        refined_sentences.append(current_sent.strip() + ".")
        current_sent = f"{conjunction.capitalize()} {next_part.strip()}"

    refined_sentences.append(current_sent.strip())
    return refined_sentences


# --- ä¾§è¾¹æ  ---
with st.sidebar:
    st.header("ğŸ“ ä½œæ–‡è¾“å…¥")
    default_text = "The rain was so big that our clothes were all wet, and we couldn't find the bus stop because it was too dark, but finally we walked home tiredly. It was a good day and we had a good time."
    text_input = st.text_area("åœ¨æ­¤ç²˜è´´ä½œæ–‡:", value=default_text, height=300)
    analyze_btn = st.button("âœ¨ å¯åŠ¨æ·±åº¦ä¼˜åŒ–")

# --- ä¸»é€»è¾‘ ---
if analyze_btn and text_input:
    blob = TextBlob(text_input)

    # ---------------------------
    # æ¨¡å—ä¸€ï¼šå…¨å±€æ•°æ®
    # ---------------------------
    st.subheader("ğŸ“Š å…¨å±€è¯Šæ–­")
    col1, col2, col3 = st.columns(3)

    words = re.findall(r'\b\w+\b', text_input.lower())
    filtered_words = [w for w in words if w not in STOP_WORDS and len(w) > 2]

    col1.metric("æƒ…æ„ŸæŒ‡æ•°", f"{blob.sentiment.polarity:.2f}")
    col2.metric("æ€»å•è¯æ•°", len(words))
    col3.metric("è¯æ±‡ä¸°å¯Œåº¦", f"{len(set(words)) / len(words):.1%}" if words else "0%")

    st.divider()

    # ---------------------------
    # æ¨¡å—äºŒï¼šè¯æ±‡ä¼˜åŒ– (å·²æ”¹å› v3.0 åˆ—è¡¨ UI)
    # ---------------------------
    st.subheader("ğŸ’¡ æ·±åº¦ä¼˜åŒ–å»ºè®® (Optimization Suggestions)")
    st.markdown("#### 1. è¯æ±‡å‡çº§ (Vocabulary Upgrade)")

    # è¿™ä¸€å—å®Œå…¨è¿˜åŸäº†ä½ æˆªå›¾ä¸­çš„è®¾è®¡ï¼šè“è‰²èƒŒæ™¯æç¤º + åˆ—è¡¨å±•ç¤º
    st.info("æ£€æµ‹ä½ ä½¿ç”¨é¢‘ç‡è¾ƒé«˜æˆ–è¿‡äºç®€å•çš„è¯ï¼Œå¹¶æ¨èé«˜çº§æ›¿æ¢è¯ï¼š")

    word_counts = Counter(filtered_words)
    common_words = word_counts.most_common(5)

    if not common_words:
        st.write("ğŸ‘ è¯æ±‡ä½¿ç”¨éå¸¸å¤šæ ·ï¼Œæ²¡æœ‰å‘ç°æ˜æ˜¾çš„é‡å¤ç”¨è¯ï¼")
    else:
        # ç”Ÿæˆåˆ—è¡¨æ•°æ®
        for word, count in common_words:
            syns = get_synonyms(word)
            if syns:
                syns_str = ", ".join(syns)
                # è¿™ç§æ ¼å¼å°±æ˜¯ä½ æƒ³è¦çš„æ•ˆæœï¼š
                st.markdown(f"- **ã€Œ{word}ã€** (ç”¨äº†{count}æ¬¡) ğŸ‘‰ å¯æ›¿æ¢ä¸º: *{syns_str}*")
            else:
                st.markdown(f"- **ã€Œ{word}ã€** (ç”¨äº†{count}æ¬¡) - æš‚æ— åŒä¹‰è¯æ¨è")

    st.markdown("<br>", unsafe_allow_html=True)

    # ---------------------------
    # æ¨¡å—ä¸‰ï¼šé•¿éš¾å¥è‡ªåŠ¨æ‹†åˆ† (ä¿ç•™ v3.1 æ‹†åˆ†åŠŸèƒ½)
    # ---------------------------
    st.markdown("#### 2. é•¿éš¾å¥æ™ºèƒ½æ‹†åˆ† (Smart Sentence Splitter)")

    LONG_SENTENCE_THRESHOLD = 20
    long_sentences = []

    for sentence in blob.sentences:
        if len(sentence.words) > LONG_SENTENCE_THRESHOLD:
            long_sentences.append(str(sentence))

    if not long_sentences:
        st.success("âœ… å¥å­ç»“æ„è‰¯å¥½ï¼Œæ²¡æœ‰å‘ç°è¿‡é•¿å¥å­ã€‚")
    else:
        st.warning(f"âš ï¸ å‘ç° {len(long_sentences)} ä¸ªé•¿éš¾å¥ï¼ŒAI ç”Ÿæˆäº†æ‹†åˆ†æ–¹æ¡ˆï¼š")

        for i, raw_sent in enumerate(long_sentences, 1):
            with st.expander(f"ğŸš© é•¿å¥ {i} (ç‚¹å‡»æŸ¥çœ‹æ‹†åˆ†ç»“æœ)", expanded=True):
                st.markdown("**ğŸ”´ åŸå¥ (Original):**")
                st.info(raw_sent)

                split_result = smart_split_sentence(raw_sent)

                st.markdown("**ğŸŸ¢ AI å»ºè®®æ‹†åˆ† (Suggested Split):**")
                if split_result:
                    for part in split_result:
                        st.success(part)
                    st.caption("ğŸ’¡ ç®—æ³•åŸç†ï¼šåŸºäºè¿è¯ (and, but, because) è¯†åˆ«é€»è¾‘æ–­ç‚¹å¹¶é‡ç»„ã€‚")
                else:
                    st.error("âŒ è¿™å¥è¯ç»“æ„å¤ªå¤æ‚ï¼ŒAI æ— æ³•è‡ªåŠ¨å®‰å…¨æ‹†åˆ†ï¼Œè¯·äººå·¥ä¿®æ”¹ã€‚")

elif not text_input:

    st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§è¾“å…¥ä½œæ–‡å¼€å§‹åˆ†æ")
